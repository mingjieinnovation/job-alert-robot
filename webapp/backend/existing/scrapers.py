"""
scrapers.py - 5ä¸ªæ¥æºæŠ“å– + ç»éªŒçº§åˆ«æ™ºèƒ½è¿‡æ»¤

æ¥æºï¼š
1. Adzuna APIï¼ˆèšåˆ Indeed/CV-Library ç­‰ï¼‰
2. Reed APIï¼ˆè‹±å›½æœ¬åœŸï¼‰
3. LinkedInï¼ˆå…¬å¼€èŒä½é¡µé¢è§£æï¼‰
4. Google Jobsï¼ˆé€šè¿‡ SerpAPIï¼‰
5. X/Twitterï¼ˆRSS Bridgeï¼‰
"""

import requests
import logging
import time
import re
import json
import xml.etree.ElementTree as ET
from datetime import datetime
from dataclasses import dataclass, field, asdict
from typing import List
from base64 import b64encode
from html.parser import HTMLParser

import config

logger = logging.getLogger(__name__)


# ============================================================
# æ•°æ®ç»“æ„
# ============================================================

@dataclass
class Job:
    title: str
    company: str
    location: str
    url: str
    source: str
    salary: str = ""
    description_snippet: str = ""
    posted_date: str = ""
    job_id: str = ""
    match_score: int = 0
    match_tags: List[str] = field(default_factory=list)
    experience_ok: bool = True

    def to_dict(self):
        d = asdict(self)
        d["match_tags"] = self.match_tags
        return d

    @property
    def unique_key(self) -> str:
        if self.job_id:
            return f"{self.source}_{self.job_id}"
        clean_title = re.sub(r'[^a-z0-9]', '', self.title.lower())
        clean_company = re.sub(r'[^a-z0-9]', '', self.company.lower())
        return f"{self.source}_{clean_company}_{clean_title}"


# ============================================================
# è¿‡æ»¤ & è¯„åˆ†
# ============================================================

def _passes_title_filter(title: str) -> bool:
    t = title.lower()
    if config.TITLE_MUST_CONTAIN:
        if not any(kw.lower() in t for kw in config.TITLE_MUST_CONTAIN):
            return False
    if config.TITLE_EXCLUDE:
        if any(kw.lower() in t for kw in config.TITLE_EXCLUDE):
            return False
    return True


def _score_job(job: Job) -> Job:
    """è¯„åˆ†ï¼šBoostå…³é”®è¯åŠ åˆ†ï¼ŒWarningå…³é”®è¯æ‰£åˆ†ï¼Œç»éªŒå¹´é™æ£€æµ‹"""
    text = (job.title + " " + job.description_snippet).lower()
    score = 0
    tags = []

    for kw in config.DESCRIPTION_BOOST_KEYWORDS:
        if kw.lower() in text:
            score += 1
            if kw.lower() in ["product analytics", "user research", "kpi",
                               "power bi", "sql", "python", "data-driven",
                               "1-3 years", "2-3 years"]:
                tags.append(f"â­{kw}")

    for kw in config.DESCRIPTION_WARNING_KEYWORDS:
        if kw.lower() in text:
            score -= 2
            tags.append(f"âš ï¸{kw}")
            job.experience_ok = False

    if any(kw in text for kw in ["genai", "generative ai", "llm", "agentic"]):
        score += 2
        tags.append("ğŸ¤–AI")
    elif " ai " in f" {text} " or "artificial intelligence" in text:
        score += 1

    years_match = re.findall(r'(\d+)\+?\s*years?', text)
    for y in years_match:
        yr = int(y)
        if yr <= 5:
            score += 1
        elif yr >= 7:
            score -= 2
            job.experience_ok = False
            tags.append(f"âš ï¸éœ€{yr}å¹´ç»éªŒ")

    job.match_score = score
    job.match_tags = tags
    return job


def _clean_html(text: str) -> str:
    """å»é™¤HTMLæ ‡ç­¾"""
    return re.sub(r'<[^>]+>', '', text or "")


# ============================================================
# 1. Adzuna API
# ============================================================

def fetch_adzuna_jobs() -> List[Job]:
    """
    Adzuna èšåˆäº† Indeed, CV-Library, Totaljobs ç­‰æ¥æº
    æ–‡æ¡£: https://developer.adzuna.com/docs/search
    å…è´¹: 250æ¬¡/å¤©
    """
    if config.ADZUNA_APP_ID == "YOUR_ADZUNA_APP_ID":
        logger.warning("  âš ï¸ Adzuna æœªé…ç½®ï¼Œè·³è¿‡")
        return []

    all_jobs = []
    base_url = f"https://api.adzuna.com/v1/api/jobs/{config.COUNTRY}/search/1"

    for query in config.SEARCH_QUERIES:
        try:
            params = {
                "app_id": config.ADZUNA_APP_ID,
                "app_key": config.ADZUNA_APP_KEY,
                "what": query,
                "where": config.LOCATION,
                "results_per_page": min(config.MAX_RESULTS_PER_QUERY, 50),  # Adzunaæœ€å¤§50
                "max_days_old": 7,
                "sort_by": "date",
                "content-type": "application/json",
            }
            if config.MIN_SALARY > 0:
                params["salary_min"] = config.MIN_SALARY

            resp = requests.get(base_url, params=params, timeout=15)
            resp.raise_for_status()
            data = resp.json()

            count = 0
            for item in data.get("results", []):
                title = item.get("title", "")
                if not _passes_title_filter(title):
                    continue

                salary = ""
                sal_min = item.get("salary_min")
                sal_max = item.get("salary_max")
                if sal_min and sal_max:
                    salary = f"Â£{int(sal_min):,} - Â£{int(sal_max):,}"
                elif sal_min:
                    salary = f"From Â£{int(sal_min):,}"

                job = Job(
                    title=title,
                    company=item.get("company", {}).get("display_name", "Unknown"),
                    location=item.get("location", {}).get("display_name", config.LOCATION),
                    url=item.get("redirect_url", ""),
                    source="adzuna",
                    salary=salary,
                    description_snippet=_clean_html(item.get("description", ""))[:500],
                    posted_date=item.get("created", "")[:10],
                    job_id=str(item.get("id", "")),
                )
                job = _score_job(job)
                all_jobs.append(job)
                count += 1

            logger.info(f"  Adzuna [{query}]: {count} ä¸ªé€šè¿‡")
            time.sleep(0.5)
        except Exception as e:
            logger.error(f"  Adzuna [{query}] å¤±è´¥: {e}")

    return all_jobs


# ============================================================
# 2. Reed API
# ============================================================

def fetch_reed_jobs() -> List[Job]:
    """
    è‹±å›½æœ¬åœŸæ±‚èŒç½‘ç«™
    æ–‡æ¡£: https://www.reed.co.uk/developers/jobseeker
    å…è´¹ï¼Œæ— é™åˆ¶
    """
    if config.REED_API_KEY == "YOUR_REED_API_KEY":
        logger.warning("  âš ï¸ Reed æœªé…ç½®ï¼Œè·³è¿‡")
        return []

    all_jobs = []
    base_url = "https://www.reed.co.uk/api/1.0/search"
    auth = b64encode(f"{config.REED_API_KEY}:".encode()).decode()

    for query in config.SEARCH_QUERIES:
        try:
            params = {
                "keywords": query,
                "locationName": config.LOCATION,
                "distancefromlocation": 15,
                "resultsToTake": min(config.MAX_RESULTS_PER_QUERY, 100),  # Reedæœ€å¤§100
            }
            if config.MIN_SALARY > 0:
                params["minimumSalary"] = config.MIN_SALARY

            resp = requests.get(base_url, params=params, timeout=15,
                                headers={"Authorization": f"Basic {auth}"})
            resp.raise_for_status()
            data = resp.json()

            count = 0
            for item in data.get("results", []):
                title = item.get("jobTitle", "")
                if not _passes_title_filter(title):
                    continue

                salary = ""
                sal_min = item.get("minimumSalary")
                sal_max = item.get("maximumSalary")
                if sal_min and sal_max:
                    salary = f"Â£{int(sal_min):,} - Â£{int(sal_max):,}"
                elif sal_min:
                    salary = f"From Â£{int(sal_min):,}"

                job = Job(
                    title=title,
                    company=item.get("employerName", "Unknown"),
                    location=item.get("locationName", config.LOCATION),
                    url=f"https://www.reed.co.uk/jobs/{item.get('jobId', '')}",
                    source="reed",
                    salary=salary,
                    description_snippet=item.get("jobDescription", "")[:500],
                    posted_date=item.get("date", "")[:10],
                    job_id=str(item.get("jobId", "")),
                )
                job = _score_job(job)
                all_jobs.append(job)
                count += 1

            logger.info(f"  Reed [{query}]: {count} ä¸ªé€šè¿‡")
            time.sleep(0.5)
        except Exception as e:
            logger.error(f"  Reed [{query}] å¤±è´¥: {e}")

    return all_jobs


# ============================================================
# 3. LinkedInï¼ˆå…¬å¼€èŒä½é¡µé¢è§£æï¼‰
# ============================================================

def fetch_linkedin_jobs() -> List[Job]:
    """
    LinkedIn å…¬å¼€èŒä½æœç´¢ï¼ˆä¸éœ€è¦ç™»å½•ï¼‰

    åŸç†ï¼šLinkedIn çš„èŒä½æœç´¢é¡µæœ‰ä¸€ä¸ªé¢å‘æœç´¢å¼•æ“çš„å…¬å¼€ç‰ˆæœ¬ï¼Œ
    é€šè¿‡ /jobs-guest/jobs/api/seeMoreJobPostings/search å¯ä»¥
    è·å– HTML æ ¼å¼çš„èŒä½åˆ—è¡¨ã€‚æˆ‘ä»¬è§£æè¿™ä¸ª HTML æå–èŒä½ä¿¡æ¯ã€‚

    æ³¨æ„ï¼šè¿™ä¸æ˜¯å®˜æ–¹ APIï¼ŒLinkedIn å¯èƒ½éšæ—¶æ”¹å˜æ ¼å¼ã€‚
    å¦‚æœæŸå¤©çªç„¶æŠ“ä¸åˆ°äº†ï¼Œä¸å½±å“å…¶ä»–æ¥æºã€‚
    """
    all_jobs = []
    base_url = "https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search"

    # LinkedIn æœç´¢è¯ç²¾ç®€ï¼ˆå¤ªå¤šä¼šè§¦å‘é¢‘ç‡é™åˆ¶ï¼‰
    linkedin_queries = [
        "AI product manager",
        "AI product analyst",
        "AI data analyst",
        "GenAI product",
        "product analytics AI",
    ]

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                       "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept-Language": "en-GB,en;q=0.9",
    }

    for query in linkedin_queries:
        try:
            params = {
                "keywords": query,
                "location": "London, United Kingdom",
                "f_TPR": "r604800",  # è¿‡å»7å¤©
                "start": 0,
                "count": min(config.MAX_RESULTS_PER_QUERY, 50),
            }

            resp = requests.get(base_url, params=params, headers=headers, timeout=15)
            if resp.status_code != 200:
                logger.warning(f"  LinkedIn [{query}]: HTTP {resp.status_code}")
                continue

            html = resp.text
            count = 0

            # è§£æ LinkedIn HTML èŒä½å¡ç‰‡
            # æ¯ä¸ªèŒä½åœ¨ <div class="base-card"> ä¸­
            card_pattern = re.compile(
                r'<div[^>]*class="[^"]*base-card[^"]*"[^>]*>.*?</div>\s*</div>\s*</div>',
                re.DOTALL
            )

            # æå–å„å­—æ®µçš„æ­£åˆ™
            title_pattern = re.compile(
                r'<h3[^>]*class="[^"]*base-search-card__title[^"]*"[^>]*>\s*(.*?)\s*</h3>',
                re.DOTALL
            )
            company_pattern = re.compile(
                r'<h4[^>]*class="[^"]*base-search-card__subtitle[^"]*"[^>]*>\s*(.*?)\s*</h4>',
                re.DOTALL
            )
            location_pattern = re.compile(
                r'<span[^>]*class="[^"]*job-search-card__location[^"]*"[^>]*>\s*(.*?)\s*</span>',
                re.DOTALL
            )
            link_pattern = re.compile(
                r'<a[^>]*class="[^"]*base-card__full-link[^"]*"[^>]*href="([^"]*)"',
                re.DOTALL
            )
            date_pattern = re.compile(
                r'<time[^>]*datetime="([^"]*)"',
                re.DOTALL
            )

            titles = title_pattern.findall(html)
            companies = company_pattern.findall(html)
            locations = location_pattern.findall(html)
            links = link_pattern.findall(html)
            dates = date_pattern.findall(html)

            num_jobs = min(len(titles), len(companies), len(links))

            for i in range(num_jobs):
                title = _clean_html(titles[i]).strip()
                if not _passes_title_filter(title):
                    continue

                company = _clean_html(companies[i]).strip() if i < len(companies) else "Unknown"
                location = _clean_html(locations[i]).strip() if i < len(locations) else "London"
                url = links[i].split("?")[0] if i < len(links) else ""
                posted = dates[i][:10] if i < len(dates) else ""

                # ä» URL æå– job ID
                job_id_match = re.search(r'/view/[^/]*-(\d+)', url)
                jid = job_id_match.group(1) if job_id_match else ""

                job = Job(
                    title=title,
                    company=company,
                    location=location,
                    url=url,
                    source="linkedin",
                    posted_date=posted,
                    job_id=jid,
                )
                job = _score_job(job)
                all_jobs.append(job)
                count += 1

            logger.info(f"  LinkedIn [{query}]: {count} ä¸ªé€šè¿‡")
            time.sleep(2)  # LinkedIn å¯¹é¢‘ç‡æ•æ„Ÿï¼Œé—´éš”é•¿ä¸€äº›

        except Exception as e:
            logger.error(f"  LinkedIn [{query}] å¤±è´¥: {e}")

    return all_jobs


# ============================================================
# 4. Google Jobsï¼ˆé€šè¿‡ SerpAPIï¼‰
# ============================================================

def fetch_google_jobs() -> List[Job]:
    """
    Google Jobs èšåˆäº† LinkedIn, Indeed, Glassdoor ç­‰æ‰€æœ‰æ¥æºã€‚

    åŸç†ï¼šGoogle æ²¡æœ‰å…¬å¼€çš„ Jobs APIï¼Œä½† SerpAPI.com æä¾›äº†ä¸€ä¸ª
    ä»£ç†æœåŠ¡ï¼Œå¯ä»¥ç»“æ„åŒ–åœ°è·å– Google Jobs çš„æœç´¢ç»“æœã€‚

    å…è´¹é¢åº¦ï¼š100æ¬¡æœç´¢/æœˆã€‚æˆ‘ä»¬æ¯å¤©ç”¨3-5æ¬¡ï¼Œä¸€ä¸ªæœˆçº¦100-150æ¬¡ï¼Œ
    åˆšå¥½åœ¨å…è´¹è¾¹ç•Œã€‚å¦‚æœè¶…äº†å¯ä»¥åªç”¨å‰å‡ ä¸ªå…³é”®è¯ã€‚

    æ³¨å†Œï¼šhttps://serpapi.com/manage-api-key
    """
    if config.SERPAPI_KEY == "YOUR_SERPAPI_KEY":
        logger.warning("  âš ï¸ SerpAPI æœªé…ç½®ï¼Œè·³è¿‡ Google Jobs")
        return []

    all_jobs = []
    base_url = "https://serpapi.com/search.json"

    # Google Jobs æœç´¢è¯ç²¾ç®€ï¼ˆèŠ‚çœ API é¢åº¦ï¼‰
    google_queries = [
        "AI product manager London",
        "AI product analyst London",
        "AI data analyst London",
    ]

    for query in google_queries:
        try:
            params = {
                "engine": "google_jobs",
                "q": query,
                "location": "London, United Kingdom",
                "api_key": config.SERPAPI_KEY,
                "chips": "date_posted:week",  # ä»…è¿‡å»ä¸€å‘¨
                "num": min(config.MAX_RESULTS_PER_QUERY, 50),
            }

            resp = requests.get(base_url, params=params, timeout=20)
            resp.raise_for_status()
            data = resp.json()

            count = 0
            for item in data.get("jobs_results", []):
                title = item.get("title", "")
                if not _passes_title_filter(title):
                    continue

                # Google Jobs çš„æè¿°æ¯”è¾ƒå®Œæ•´ï¼Œæœ‰åˆ©äºè¯„åˆ†
                desc = item.get("description", "")[:500]

                # æå–è–ªèµ„ï¼ˆå¦‚æœæœ‰ï¼‰
                salary = ""
                detected = item.get("detected_extensions", {})
                if detected.get("salary"):
                    salary = detected["salary"]

                # æå–ç”³è¯·é“¾æ¥
                url = ""
                apply_options = item.get("apply_options", [])
                if apply_options:
                    url = apply_options[0].get("link", "")

                # æå–å‘å¸ƒæ—¶é—´
                posted = detected.get("posted_at", "")

                job = Job(
                    title=title,
                    company=item.get("company_name", "Unknown"),
                    location=item.get("location", "London"),
                    url=url,
                    source="google_jobs",
                    salary=salary,
                    description_snippet=desc,
                    posted_date=posted,
                    job_id=item.get("job_id", ""),
                )
                job = _score_job(job)
                all_jobs.append(job)
                count += 1

            logger.info(f"  Google Jobs [{query}]: {count} ä¸ªé€šè¿‡")
            time.sleep(1)

        except Exception as e:
            logger.error(f"  Google Jobs [{query}] å¤±è´¥: {e}")

    return all_jobs


# ============================================================
# 5. X/Twitterï¼ˆRSS Bridgeï¼‰
# ============================================================

def fetch_x_jobs() -> List[Job]:
    """
    é€šè¿‡å…¬å¼€ RSS Bridge æŠ“å– X/Twitter æ‹›è˜å¸–ã€‚
    ç¬¬ä¸‰æ–¹æœåŠ¡ï¼Œå¯èƒ½ä¸ç¨³å®šï¼Œä½œä¸ºè¡¥å……æ¥æºã€‚
    """
    all_jobs = []

    x_queries = [
        '"hiring" "London" "AI" (product OR analyst)',
        'from:illbeback_jobs London',
        'from:Egooshift analyst London',
        'from:AIJobAlert London',
    ]

    rss_bridges = [
        "https://rss-bridge.org/bridge01",
        "https://rss-bridge.bb8.fun",
    ]

    for query in x_queries:
        fetched = False
        for bridge in rss_bridges:
            if fetched:
                break
            try:
                params = {
                    "action": "display",
                    "bridge": "TwitterBridge",
                    "context": "By+keyword",
                    "q": query,
                    "format": "Mrss",
                }
                resp = requests.get(f"{bridge}/", params=params, timeout=15,
                                    headers={"User-Agent": "JobAlertBot/1.0"})
                if resp.status_code != 200:
                    continue

                root = ET.fromstring(resp.content)
                for item in root.findall('.//item')[:5]:
                    title_el = item.find('title')
                    link_el = item.find('link')
                    desc_el = item.find('description')

                    if title_el is None or link_el is None:
                        continue

                    text = title_el.text or ""
                    if not any(kw in text.lower() for kw in
                               ['hiring', 'job', 'role', 'position', 'vacancy', 'looking for']):
                        continue
                    if any(kw in text.lower() for kw in ['intern', 'director', 'vp', 'head of']):
                        continue

                    job = Job(
                        title=f"[X] {text[:120]}",
                        company="(via X/Twitter)",
                        location="London",
                        url=link_el.text or "",
                        source="x_twitter",
                        description_snippet=_clean_html(desc_el.text if desc_el is not None else "")[:300],
                        job_id=link_el.text or "",
                    )
                    job = _score_job(job)
                    all_jobs.append(job)

                fetched = True
                logger.info(f"  X [{query[:35]}...]: æˆåŠŸ")
            except Exception:
                continue

        if not fetched:
            logger.debug(f"  X [{query[:35]}...]: ä¸å¯ç”¨")
        time.sleep(1)

    return all_jobs


# ============================================================
# ç»Ÿä¸€å…¥å£
# ============================================================

def fetch_all_jobs() -> List[Job]:
    all_jobs = []

    logger.info("=" * 55)
    logger.info("ğŸ” å¼€å§‹æŠ“å–ï¼ˆ5ä¸ªæ¥æºï¼‰...")

    sources = [
        ("ğŸ“¡ 1/5 Adzunaï¼ˆèšåˆ Indeed/CV-Library ç­‰ï¼‰", fetch_adzuna_jobs),
        ("ğŸ“¡ 2/5 Reedï¼ˆè‹±å›½æœ¬åœŸï¼‰", fetch_reed_jobs),
        ("ğŸ“¡ 3/5 LinkedInï¼ˆå…¬å¼€èŒä½ï¼‰", fetch_linkedin_jobs),
        ("ğŸ“¡ 4/5 Google Jobsï¼ˆèšåˆæ‰€æœ‰å¹³å°ï¼‰", fetch_google_jobs),
        ("ğŸ“¡ 5/5 X/Twitter", fetch_x_jobs),
    ]

    for label, fetcher in sources:
        logger.info(f"\n{label}")
        try:
            jobs = fetcher()
            all_jobs.extend(jobs)
            logger.info(f"   â†’ {len(jobs)} ä¸ªé€šè¿‡è¿‡æ»¤")
        except Exception as e:
            logger.error(f"   â†’ æ¥æºå¤±è´¥: {e}")

    # æŒ‰åŒ¹é…åˆ†æ•°æ’åº
    all_jobs.sort(key=lambda j: j.match_score, reverse=True)

    logger.info(f"\nğŸ¯ 5ä¸ªæ¥æºæ€»è®¡: {len(all_jobs)} ä¸ªï¼ˆæœªå»é‡ï¼‰")
    return all_jobs
